import os
import warnings
import numpy as np
import cv2
import torch
import msgpack
import msgpack_numpy as m

# Assuming sam2 is installed and accessible
# If sam2 is a local module, adjust the import path accordingly
try:
    from sam2.build_sam import build_sam2
    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
except ImportError:
    print("Warning: sam2 library not found. Segmentation functions will not work.")
    build_sam2 = None
    SAM2AutomaticMaskGenerator = None

# Register numpy's serialization handler for msgpack
m.patch()

# Ignore warnings (consider more specific warning handling in production)
warnings.filterwarnings("ignore")

def image_segmentation(image_path, sam_model_cfg, sam_checkpoint_path, device_str="cuda"):
    """
    Performs image segmentation using SAM2 model.

    Args:
        image_path (str): Path to the input image file.
        sam_model_cfg (str): Path to the SAM2 model configuration YAML file.
        sam_checkpoint_path (str): Path to the SAM2 model checkpoint file (.pt).
        device_str (str): Device to run the model on ('cuda' or 'cpu').

    Returns:
        tuple: A tuple containing:
            - list: List of masks generated by SAM2. Each mask is a dictionary.
            - np.ndarray: The preprocessed image used for segmentation.
    Raises:
        ImportError: If sam2 library is not installed.
        FileNotFoundError: If image, config or checkpoint file not found.
        RuntimeError: If CUDA device is requested but not available.
    """
    if build_sam2 is None or SAM2AutomaticMaskGenerator is None:
        raise ImportError("sam2 library is required for image segmentation.")

    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    if not os.path.exists(sam_model_cfg):
        raise FileNotFoundError(f"SAM2 config file not found: {sam_model_cfg}")
    if not os.path.exists(sam_checkpoint_path):
        raise FileNotFoundError(f"SAM2 checkpoint file not found: {sam_checkpoint_path}")

    device = torch.device(device_str)
    if device_str == "cuda" and not torch.cuda.is_available():
        raise RuntimeError("CUDA device requested but not available.")

    # Build SAM2 model
    sam2 = build_sam2(sam_model_cfg, sam_checkpoint_path, device=device, apply_postprocessing=True)

    # Configure mask generator (parameters from the notebook)
    mask_generator = SAM2AutomaticMaskGenerator(
        model=sam2,
        points_per_side=64,
        points_per_batch=128,
        pred_iou_thresh=0.75,
        stability_score_thresh=0.92,
        stability_score_offset=0.7,
        crop_n_layers=0,
        box_nms_thresh=0.5,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=50.0, # Note: deduplicate_masks has its own min_area
        use_m2m=True,
    )

    # Load and preprocess image
    image = cv2.imread(image_path)
    if image is None:
        raise IOError(f"Could not read image file: {image_path}")

    # Preprocessing steps from notebook
    image = image[20:] # Crop top 20 pixels
    if image.shape[-1] == 4: # Handle RGBA images
        image = image[:, :, :3]
    image = image[:850, :, :] # Crop height to 850 pixels
    image = image.astype(np.uint8)

    # Generate masks
    masks = mask_generator.generate(image)

    return masks, image


def deduplicate_masks(masks, image_shape, min_area=100, area_ratio=None):
    """
    Deduplicates masks by removing overlaps and filtering small areas.
    Larger masks have priority in claiming pixels.

    Args:
        masks (list): List of mask dictionaries, each containing a 'segmentation'
                      boolean numpy array.
        image_shape (tuple): Shape of the original image (H, W) or (H, W, C).
        min_area (int): Minimum area threshold in pixels. Defaults to 100.
        area_ratio (float, optional): Minimum area threshold as a ratio (0-1)
                                      of the total image area. If provided,
                                      the effective min_area will be the maximum
                                      of the pixel threshold and the ratio threshold.
                                      Defaults to None.

    Returns:
        list: List of deduplicated mask dictionaries. The 'segmentation'
              array is updated to remove overlaps, and 'area' is updated
              if present.
    """
    if not masks:
        return []

    # Sort masks by area (descending) to prioritize larger masks
    # Ensure 'segmentation' key exists and is a numpy array
    valid_masks = [m for m in masks if 'segmentation' in m and isinstance(m['segmentation'], np.ndarray)]
    if not valid_masks:
        print("Warning: No valid masks found for deduplication.")
        return []

    sorted_masks = sorted(valid_masks, key=lambda x: np.sum(x['segmentation']), reverse=True)

    dedup_masks = []
    combined_mask = np.zeros(image_shape[:2], dtype=bool) # Use only H, W for the combined mask
    total_pixels = image_shape[0] * image_shape[1]

    # Calculate effective minimum area threshold
    effective_min_area = min_area
    if area_ratio is not None:
        effective_min_area = max(min_area, int(total_pixels * area_ratio))

    for mask_dict in sorted_masks:
        original_mask = mask_dict['segmentation']
        # Ensure mask dimensions match image_shape[:2]
        if original_mask.shape != image_shape[:2]:
             print(f"Warning: Skipping mask with mismatched shape {original_mask.shape} vs image {image_shape[:2]}")
             continue

        # Remove pixels already claimed by larger masks
        new_mask_array = original_mask & ~combined_mask
        current_area = np.sum(new_mask_array)

        # Filter by area
        if current_area >= effective_min_area:
            # Create a copy to avoid modifying the original input list
            dedup_mask_dict = mask_dict.copy()
            dedup_mask_dict['segmentation'] = new_mask_array

            # Update area if it exists in the dictionary
            if 'area' in dedup_mask_dict:
                dedup_mask_dict['area'] = current_area
            # If 'area' doesn't exist, add it
            else:
                 dedup_mask_dict['area'] = current_area


            # Add the non-overlapping part to the combined mask
            combined_mask |= new_mask_array
            dedup_masks.append(dedup_mask_dict)

    return dedup_masks


def save_masks(masks, save_path):
    """
    Saves a list of mask dictionaries to a file using msgpack.

    Args:
        masks (list): List of mask dictionaries. Expected keys like
                      'segmentation', 'area', 'bbox', etc. are preserved.
                      'segmentation' should be a numpy array.
        save_path (str): Path to save the output file.
    """
    # Ensure the output directory exists
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    # Prepare data for serialization (ensure numpy arrays are handled)
    # msgpack_numpy handles numpy arrays automatically if m.patch() is called
    serializable_masks = []
    for mask in masks:
         # Ensure essential keys are present or add defaults if necessary
         m_copy = mask.copy() # Work on a copy
         if 'segmentation' not in m_copy or not isinstance(m_copy['segmentation'], np.ndarray):
             print(f"Warning: Skipping mask due to missing or invalid 'segmentation' key in save_masks.")
             continue
         # Add area if missing
         if 'area' not in m_copy:
              m_copy['area'] = int(np.sum(m_copy['segmentation'])) # Calculate and cast to standard int
         else:
              # Ensure area is a standard Python int/float for broader compatibility
              m_copy['area'] = float(m_copy['area']) if isinstance(m_copy['area'], np.generic) else m_copy['area']

         # Ensure other common fields are serializable
         if 'bbox' in m_copy:
              m_copy['bbox'] = [float(x) if isinstance(x, np.generic) else x for x in m_copy['bbox']]
         if 'predicted_iou' in m_copy:
              m_copy['predicted_iou'] = float(m_copy['predicted_iou']) if isinstance(m_copy['predicted_iou'], np.generic) else m_copy['predicted_iou']
         if 'stability_score' in m_copy:
              m_copy['stability_score'] = float(m_copy['stability_score']) if isinstance(m_copy['stability_score'], np.generic) else m_copy['stability_score']

         serializable_masks.append(m_copy)


    # Serialize using msgpack
    try:
        with open(save_path, 'wb') as f:
            packed = msgpack.packb(serializable_masks, use_bin_type=True)
            f.write(packed)
    except Exception as e:
        print(f"Error saving masks to {save_path}: {e}")
        raise


def load_masks(load_path):
    """
    Loads a list of mask dictionaries from a file saved using msgpack.

    Args:
        load_path (str): Path to the mask file.

    Returns:
        list: List of loaded mask dictionaries. Numpy arrays are automatically
              reconstructed by msgpack_numpy.
    Raises:
        FileNotFoundError: If the load_path does not exist.
        Exception: If there is an error during deserialization.
    """
    if not os.path.exists(load_path):
        raise FileNotFoundError(f"Mask file not found: {load_path}")

    try:
        with open(load_path, 'rb') as f:
            packed = f.read()
        # Use raw=False is important for decoding strings correctly
        unpacked_data = msgpack.unpackb(packed, raw=False)
        return unpacked_data
    except Exception as e:
        print(f"Error loading masks from {load_path}: {e}")
        raise


def batch_save_masks(input_dir, output_dir, sam_model_cfg, sam_checkpoint_path, device_str="cuda", file_extension=".tif", min_area=100, area_ratio=None):
    """
    Processes a batch of images in an input directory, performs segmentation,
    deduplicates masks, and saves the results to an output directory,
    preserving the subdirectory structure.

    Args:
        input_dir (str): Path to the root directory containing image files.
        output_dir (str): Path to the root directory where .mask files will be saved.
        sam_model_cfg (str): Path to the SAM2 model configuration YAML file.
        sam_checkpoint_path (str): Path to the SAM2 model checkpoint file (.pt).
        device_str (str): Device for SAM2 model ('cuda' or 'cpu'). Defaults to "cuda".
        file_extension (str): Extension of image files to process (e.g., '.tif', '.png').
                               Defaults to ".tif".
        min_area (int): Minimum area threshold for deduplication. Defaults to 100.
        area_ratio (float, optional): Minimum area ratio for deduplication. Defaults to None.
    """
    print(f"Starting batch mask generation from '{input_dir}' to '{output_dir}'...")
    processed_files = 0
    failed_files = 0

    for root, dirs, files in os.walk(input_dir):
        print(f"Scanning directory: {root}")
        for file in files:
            if file.endswith(file_extension):
                img_path = os.path.join(root, file)
                print(f"Processing image: {img_path}...")

                try:
                    # Perform segmentation
                    masks, image = image_segmentation(img_path, sam_model_cfg, sam_checkpoint_path, device_str)

                    # Deduplicate masks
                    dedup_masks = deduplicate_masks(masks, image.shape[:2], min_area=min_area, area_ratio=area_ratio)

                    if not dedup_masks:
                         print(f"  No valid masks found after deduplication for {file}. Skipping save.")
                         continue


                    # Determine save path, preserving structure
                    relative_root = os.path.relpath(root, input_dir)
                    save_dir = os.path.join(output_dir, relative_root)
                    os.makedirs(save_dir, exist_ok=True)

                    base_name = os.path.splitext(file)[0]
                    save_path = os.path.join(save_dir, f"{base_name}.mask")

                    # Save the deduplicated masks
                    save_masks(dedup_masks, save_path)
                    print(f"  Successfully saved masks to: {save_path}")
                    processed_files += 1

                except (ImportError, FileNotFoundError, RuntimeError, IOError, Exception) as e:
                    print(f"  Failed to process {img_path}: {e}")
                    failed_files += 1
                finally:
                    # Clean up GPU memory if applicable
                    if device_str == "cuda":
                        torch.cuda.empty_cache()


    print(f"\nBatch processing complete.")
    print(f"  Successfully processed: {processed_files} files.")
    print(f"  Failed to process: {failed_files} files.")

# Example usage (if run as a script)
if __name__ == '__main__':
    # THESE PATHS ARE EXAMPLES AND MUST BE SET CORRECTLY
    # Consider using argparse or a config file in a real application
    INPUT_IMAGE_DIR = r'/home/kemove/Desktop/Mag/2-ZH/0-ImageProcessing/20250228' # Example path from notebook
    OUTPUT_MASK_DIR = r'/home/kemove/Desktop/Mag/2-ZH/0-ImageProcessing/20250228/masks' # Example path from notebook
    SAM_CHECKPOINT = r"/home/kemove/Desktop/Github/sam2/checkpoints/sam2.1_hiera_large.pt" # Example path from notebook
    SAM_CONFIG = "configs/sam2.1/sam2.1_hiera_l.yaml" # Example path from notebook (relative path might need adjustment)
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    print("Running segmentation module example...")
    print(f"Input directory: {INPUT_IMAGE_DIR}")
    print(f"Output directory: {OUTPUT_MASK_DIR}")
    print(f"SAM Checkpoint: {SAM_CHECKPOINT}")
    print(f"SAM Config: {SAM_CONFIG}")
    print(f"Device: {DEVICE}")

    # Check if example paths exist before running
    if not os.path.isdir(INPUT_IMAGE_DIR):
        print(f"Error: Input directory '{INPUT_IMAGE_DIR}' not found. Cannot run example.")
    elif not os.path.exists(SAM_CHECKPOINT):
         print(f"Error: SAM checkpoint '{SAM_CHECKPOINT}' not found. Cannot run example.")
    # Add check for SAM_CONFIG if it's a required absolute path
    # elif not os.path.exists(SAM_CONFIG):
    #      print(f"Error: SAM config '{SAM_CONFIG}' not found. Cannot run example.")
    else:
        try:
            # Create output directory if it doesn't exist
            os.makedirs(OUTPUT_MASK_DIR, exist_ok=True)

            batch_save_masks(
                input_dir=INPUT_IMAGE_DIR,
                output_dir=OUTPUT_MASK_DIR,
                sam_model_cfg=SAM_CONFIG,
                sam_checkpoint_path=SAM_CHECKPOINT,
                device_str=DEVICE,
                file_extension=".tif" # Assuming .tif images as per notebook
            )
        except Exception as main_e:
            print(f"An error occurred during the example run: {main_e}")

    print("Segmentation module example finished.")
